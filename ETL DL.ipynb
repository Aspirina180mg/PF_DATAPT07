{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de datos Datasets Google Maps y YELP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import geojson\n",
    "from shapely.geometry import Point, Polygon, shape\n",
    "from shapely.errors import TopologicalError\n",
    "from fuzzywuzzy import process, fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiciones y carpetas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de diccionario de Estados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dictionary = {\n",
    "    \"AL\": \"Alabama\",\n",
    "    \"AK\": \"Alaska\",\n",
    "    \"AZ\": \"Arizona\",\n",
    "    \"AR\": \"Arkansas\",\n",
    "    \"CA\": \"California\",\n",
    "    \"CO\": \"Colorado\",\n",
    "    \"CT\": \"Connecticut\",\n",
    "    \"DE\": \"Delaware\",\n",
    "    \"FL\": \"Florida\",\n",
    "    \"GA\": \"Georgia\",\n",
    "    \"HI\": \"Hawaii\",\n",
    "    \"ID\": \"Idaho\",\n",
    "    \"IL\": \"Illinois\",\n",
    "    \"IN\": \"Indiana\",\n",
    "    \"IA\": \"Iowa\",\n",
    "    \"KS\": \"Kansas\",\n",
    "    \"KY\": \"Kentucky\",\n",
    "    \"LA\": \"Louisiana\",\n",
    "    \"ME\": \"Maine\",\n",
    "    \"MD\": \"Maryland\",\n",
    "    \"MA\": \"Massachusetts\",\n",
    "    \"MI\": \"Michigan\",\n",
    "    \"MN\": \"Minnesota\",\n",
    "    \"MS\": \"Mississippi\",\n",
    "    \"MO\": \"Missouri\",\n",
    "    \"MT\": \"Montana\",\n",
    "    \"NE\": \"Nebraska\",\n",
    "    \"NV\": \"Nevada\",\n",
    "    \"NH\": \"New Hampshire\",\n",
    "    \"NJ\": \"New Jersey\",\n",
    "    \"NM\": \"New Mexico\",\n",
    "    \"NY\": \"New York\",\n",
    "    \"NC\": \"North Carolina\",\n",
    "    \"ND\": \"North Dakota\",\n",
    "    \"OH\": \"Ohio\",\n",
    "    \"OK\": \"Oklahoma\",\n",
    "    \"OR\": \"Oregon\",\n",
    "    \"PA\": \"Pennsylvania\",\n",
    "    \"RI\": \"Rhode Island\",\n",
    "    \"SC\": \"South Carolina\",\n",
    "    \"SD\": \"South Dakota\",\n",
    "    \"TN\": \"Tennessee\",\n",
    "    \"TX\": \"Texas\",\n",
    "    \"UT\": \"Utah\",\n",
    "    \"VT\": \"Vermont\",\n",
    "    \"VA\": \"Virginia\",\n",
    "    \"WA\": \"Washington\",\n",
    "    \"WV\": \"West Virginia\",\n",
    "    \"WI\": \"Wisconsin\",\n",
    "    \"WY\": \"Wyoming\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diccionario de geolocacización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Geojson/us-states.json\", \"r\") as f: \n",
    "    data_geo_1 = geojson.load(f)\n",
    "with open(\"Geojson/geojson-counties-fips.json\", \"r\") as f:\n",
    "    data_geo_2 = geojson.load(f)\n",
    "\n",
    "\n",
    "def get_state(lat, lon):\n",
    "    def search_states(data_geo, state_dict):\n",
    "        for feature in data_geo[\"features\"]:\n",
    "            try:\n",
    "                geometry_type = feature[\"geometry\"][\"type\"]\n",
    "                if geometry_type == \"Polygon\":\n",
    "                    coordinates = feature[\"geometry\"][\"coordinates\"][0]\n",
    "                elif geometry_type == \"MultiPolygon\":\n",
    "                    coordinates = [\n",
    "                        sub_coords[0]\n",
    "                        for sub_coords in feature[\"geometry\"][\"coordinates\"]]\n",
    "                else: continue\n",
    "                polygon = Polygon(coordinates)\n",
    "                if polygon.contains(Point(lon, lat)):\n",
    "                    state_abbr = feature[\"properties\"][\"STATE\"]\n",
    "                    return state_dict[state_abbr]\n",
    "            except (KeyError, IndexError): pass\n",
    "        return None\n",
    "\n",
    "    state = search_states(data_geo_1, state_dictionary_inv)\n",
    "    if state: return state\n",
    "    state = search_states(data_geo_2, state_index_inv)\n",
    "    return state\n",
    "\n",
    "\n",
    "def get_state_ab(address):\n",
    "    try:\n",
    "        state = address.split(\", \")[-1].split(\" \")[0]\n",
    "        if state in list(state_dictionary.keys()): return state\n",
    "        else: return np.nan\n",
    "    except: return np.nan\n",
    "\n",
    "\n",
    "state_index = pd.read_parquet(\"Datalake/estados.parquet\")[\"State\"].to_dict()\n",
    "state_index_inv = {v: k for k, v in state_index.items()}\n",
    "state_dictionary_inv = {v: k for k, v in state_dictionary.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de directorios si no existen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"Datalake\") if not os.path.exists(\"Datalake\") else None\n",
    "os.chdir(\"Datalake\")\n",
    "for subdir in [\"Google\", \"Yelp\"]:\n",
    "    os.mkdir(subdir) if not os.path.exists(subdir) else None\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de los datasets de Google Maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata de Sitios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se recorren los archivos línea por línea, durante el proceso se almacenan sólo las filas que incluyan <code>Restaurant</code> en la columna de categoría, así se optimiza el tamaño del dataset final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas_json = []\n",
    "\n",
    "for i in range(1, 12):\n",
    "    path = f\"Datasets/Google Maps/metadata-sitios/{i}.json\"\n",
    "    try:\n",
    "        with open(path, \"r\") as file:\n",
    "            for l in file:\n",
    "                try:\n",
    "                    linea_j = json.loads(l)\n",
    "                    if \"restaurant\" in \" \".join(linea_j[\"category\"]).lower():\n",
    "                        lineas_json.append(linea_j)\n",
    "                except: pass\n",
    "    except FileNotFoundError: pass\n",
    "\n",
    "if lineas_json:\n",
    "    df = pd.DataFrame(lineas_json)\n",
    "    df_maps_restaurantes = df\n",
    "else: print(\"No se encontraron datos para procesar.\")\n",
    "\n",
    "# eliminación de locales permanentemente cerrados\n",
    "df_maps_restaurantes = df_maps_restaurantes[\n",
    "    df_maps_restaurantes[\"state\"] != \"Permanently closed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de información de Estados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base al campo <code>Address</code> obtenemos el estado donde se encuentra el negocio. Nos servirá para luego seleccionar los estados con más restaurantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_ab(st):\n",
    "    try:\n",
    "        state = st.split(\", \")[-1].split(\" \")[0]\n",
    "        if state in list(state_dictionary.keys()): return state\n",
    "        else: return np.nan\n",
    "    except: return np.nan\n",
    "\n",
    "\n",
    "df[\"state_ab\"] = df[\"address\"].apply(get_state_ab)\n",
    "top_5 = df[\"state_ab\"].value_counts().head(5).index.to_list()\n",
    "df[\"us_state\"] = df[\"state_ab\"].map(state_dictionary)\n",
    "\n",
    "top_5_url = [\n",
    "    f\"Datasets/Google Maps/reviews-estados/review-{state_dictionary[i].replace(' ', '_')}/\"\n",
    "    for i in top_5]\n",
    "\n",
    "cantidad_archivos = {}\n",
    "for i in top_5_url: \n",
    "\tfor j in os.walk(i): cantidad_archivos[i] = len(j[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Estados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con los estados elegidos estamos en condiciones de ingestar los datos de las carpetas correspondientes dentro del directorio <code>reviews-estados</code>.\n",
    "Es información masiva lo que genera un archivo de grandes dimensiones, sin embargo previamente filtramos por el parámetro de año <code>2017-2019</code> valiéndonos del campo <code>time</code>, que tiene es un <code>timestamp</code>, pero con 3 digitos más que el usado por <code>datetime</code> de Python. Le agregamos el campo <code>Estado</code> que es más descriptivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas_json_revs_google = []\n",
    "for i in top_5_url:\n",
    "    count = 0\n",
    "    for c in range(1, cantidad_archivos[i] + 1):\n",
    "        file_name = f\"{i}{c}.json\"\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                for s in f:\n",
    "                    try:\n",
    "                        linea = json.loads(s)\n",
    "                        linea[\"anio\"] = datetime.datetime.fromtimestamp(\n",
    "                            linea[\"time\"] / 1000).year\n",
    "                        linea[\"estado\"] = i.split(\"-\")[-1][:-1]\n",
    "                        lineas_json_revs_google.append(linea)\n",
    "                    except: pass\n",
    "    count += 1\n",
    "\n",
    "df_revs_google = pd.DataFrame(lineas_json_revs_google)\n",
    "merge_site_reviews = pd.merge(df_revs_google, df, left_on=\"gmap_id\", right_on=\"gmap_id\")\n",
    "df_maps_reviews = merge_site_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de los Dataset de YELP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiene los datos de las entidades negocios de Yelp, a un primer vistazo tiene las columnas duplicadas, por lo que hay que hacer un recorte, ya que la segunda mitad tiene datos vacíos en su inmensa mayoría.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business = pd.read_pickle(r\"Datasets\\Yelp\\business.pkl\")\n",
    "df_business = df_business.iloc[:, :-14]\n",
    "df_business = df_business[df_business.state.isin(top_5)]\n",
    "\n",
    "\n",
    "def is_restaurant(st):\n",
    "    try:\n",
    "        test = \"\".join(st).lower()\n",
    "        return \"restaurant\" in test\n",
    "    except: return False\n",
    "\n",
    "\n",
    "df_business = df_business[df_business[\"categories\"].apply(is_restaurant)]\n",
    "df_yelp_restaurantes = df_business\n",
    "\n",
    "# eliminación de locales cerrados\n",
    "df_yelp_restaurantes = df_yelp_restaurantes[df_yelp_restaurantes[\"is_open\"] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas_json_review = []\n",
    "\n",
    "with open(r\"Datasets\\Yelp\\review.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    count = 0\n",
    "    for i in f:\n",
    "        try:\n",
    "            linea = json.loads(i)\n",
    "            if linea[\"useful\"] == 1: lineas_json_review.append(linea)\n",
    "        except: pass\n",
    "\n",
    "df_reviews = pd.DataFrame(lineas_json_review)\n",
    "df_reviews[\"funny\"] = df_reviews[\"funny\"].astype(\"int8\")\n",
    "df_reviews[\"stars\"] = df_reviews[\"stars\"].astype(\"int8\")\n",
    "df_reviews[\"cool\"] = df_reviews[\"cool\"].astype(\"int8\")\n",
    "df_reviews.drop(\"useful\", axis=1, inplace=True, errors=\"ignore\")\n",
    "\n",
    "df_yelp_reviews = df_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de columnas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mantendrán sólo las columnas relacionadas al Modelo Entidad Relación, al mismo tiempo, serán renombradas para tener un formato unificado, que permita la unión en un sólo dataframe según las tablas del MER.\n",
    "\n",
    "- Se crea df_maps_reviews.review_id tomando los primeros 10 caracteres de gmap_id y user_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maps_restaurantes = df_maps_restaurantes[[\n",
    "        \"name\",\n",
    "        \"gmap_id\",\n",
    "        \"category\",\n",
    "        \"num_of_reviews\",\n",
    "        \"state\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"MISC\",\n",
    "        \"avg_rating\",]]\n",
    "df_maps_restaurantes = df_maps_restaurantes.rename(\n",
    "    columns={\n",
    "        \"name\": \"nombre\",\n",
    "        \"gmap_id\": \"id_restaurante\",\n",
    "        \"category\": \"categorias\",\n",
    "        \"num_of_reviews\": \"cantidad_resenas\",\n",
    "        \"state\": \"estado\",\n",
    "        \"latitude\": \"latitud\",\n",
    "        \"longitude\": \"longitud\",\n",
    "        \"MISC\": \"atributos\",\n",
    "        \"avg_rating\": \"calificacion\",})\n",
    "\n",
    "df_yelp_restaurantes = df_yelp_restaurantes[[\n",
    "        \"name\",\n",
    "        \"business_id\",\n",
    "        \"categories\",\n",
    "        \"review_count\",\n",
    "        \"state\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        \"attributes\",\n",
    "        \"stars\",]]\n",
    "df_yelp_restaurantes = df_yelp_restaurantes.rename(\n",
    "    columns={\n",
    "        \"name\": \"nombre\",\n",
    "        \"business_id\": \"id_restaurante\",\n",
    "        \"categories\": \"categorias\",\n",
    "        \"review_count\": \"cantidad_resenas\",\n",
    "        \"state\": \"estado\",\n",
    "        \"latitude\": \"latitud\",\n",
    "        \"longitude\": \"longitud\",\n",
    "        \"attributes\": \"atributos\",\n",
    "        \"stars\": \"calificacion\",\n",
    "    }\n",
    ")\n",
    "\n",
    "df_maps_reviews[\"review_id\"] = (\n",
    "    df_maps_reviews[\"gmap_id\"].str[:10] + df_maps_reviews[\"user_id\"].str[:10])\n",
    "df_maps_reviews[\"sentiment_score\"] = 100\n",
    "df_maps_reviews = df_maps_reviews[[\n",
    "        \"user_id\",\n",
    "        \"gmap_id\",\n",
    "        \"review_id\",\n",
    "        \"rating\",\n",
    "        \"anio\",\n",
    "        \"text\",\n",
    "        \"sentiment_score\",]]\n",
    "df_maps_reviews = df_maps_reviews.rename(\n",
    "    columns={\n",
    "        \"user_id\": \"id_usuario\",\n",
    "        \"gmap_id\": \"id_restaurante\",\n",
    "        \"review_id\": \"id_resena\",\n",
    "        \"rating\": \"calificacion\",\n",
    "        \"anio\": \"anio\",\n",
    "        \"text\": \"texto\",\n",
    "        \"sentiment_score\": \"puntaje_de_sentimiento\",})\n",
    "\n",
    "df_yelp_reviews[\"anio\"] = df_yelp_reviews[\"date\"].str[:4]\n",
    "df_yelp_reviews[\"sentiment_score\"] = 100\n",
    "df_yelp_reviews = df_yelp_reviews[[\n",
    "        \"user_id\",\n",
    "        \"business_id\",\n",
    "        \"review_id\",\n",
    "        \"stars\",\n",
    "        \"anio\",\n",
    "        \"text\",\n",
    "        \"sentiment_score\",]]\n",
    "df_yelp_reviews = df_yelp_reviews.rename(\n",
    "    columns={\n",
    "        \"user_id\": \"id_usuario\",\n",
    "        \"business_id\": \"id_restaurante\",\n",
    "        \"review_id\": \"id_resena\",\n",
    "        \"stars\": \"calificacion\",\n",
    "        \"anio\": \"anio\",\n",
    "        \"text\": \"texto\",\n",
    "        \"sentiment_score\": \"puntaje_de_sentimiento\",})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unión de Dataframes y limpieza de duplicados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se unirán los Dataframes de restaurantes, y los dataframes de reviews, además de esto, se hará una revisión de los posibles registros duplicados y se eliminarán, esto apoyandose en los datos de longitud y latitud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurantes = pd.concat([df_yelp_restaurantes, df_maps_restaurantes])\n",
    "df_restaurantes = df_restaurantes.drop_duplicates(subset=[\"id_restaurante\"])\n",
    "\n",
    "\n",
    "# Corrección de columna categorias\n",
    "def convert_to_string(value):\n",
    "    if isinstance(value, np.ndarray): return \", \".join(map(str, value))\n",
    "    return str(value)\n",
    "\n",
    "df_restaurantes[\"categorias\"] = df_restaurantes[\"categorias\"].apply(convert_to_string)\n",
    "\n",
    "df_reviews = pd.concat([df_yelp_reviews, df_maps_reviews])\n",
    "df_reviews = df_reviews.drop_duplicates(subset=[\"id_resena\"])\n",
    "# corrección de columna anio\n",
    "df_reviews[\"anio\"] = df_reviews[\"anio\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_duplicates(df, name_col, lat_col, lon_col, threshold=90):\n",
    "    duplicates = []\n",
    "    seen = {}\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        lat_lon = (float(row[lat_col]), float(row[lon_col]))\n",
    "        if lat_lon in seen:\n",
    "            # Chequear similitud de nombre\n",
    "            result = process.extractOne(row[name_col], seen[lat_lon], scorer=fuzz.ratio)\n",
    "            if result is not None:\n",
    "                match, score = result[0], result[1]\n",
    "                if score >= threshold: duplicates.append(index)\n",
    "                else: seen[lat_lon].append(row[name_col])\n",
    "        else: seen[lat_lon] = [row[name_col]]\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "duplicate_indices = find_similar_duplicates(\n",
    "    df_restaurantes, \"nombre\", \"latitud\", \"longitud\")\n",
    "df_restaurantes = df_restaurantes.drop(index=duplicate_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de esto, se hace un conteo de las \"categorías\" en las que está clasificado Subway, y se descartan todos los resturantes que no entren dentro del 20% que más se repite, esto aplicando la ley de pareto.\n",
    "Además se cruzan los registros, junto a los de reviews, para eliminar los reviews no relacionados a restaurantes válidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "subway_restaurants = df_restaurantes[\n",
    "    df_restaurantes[\"nombre\"].str.contains(\"subway\", case=False, na=False)]\n",
    "categorias_uniques = subway_restaurants[\"categorias\"].unique()\n",
    "categorias_separadas = [\n",
    "    categorias.strip(\"][\").split(\", \") for categorias in categorias_uniques]\n",
    "conteo_categorias = {}\n",
    "for categorias_lista in categorias_separadas:\n",
    "    for categoria in categorias_lista:\n",
    "        categoria_sin_comillas = categoria.strip(\"'\")\n",
    "        if categoria_sin_comillas in conteo_categorias: \n",
    "            conteo_categorias[categoria_sin_comillas] += 1\n",
    "        else: conteo_categorias[categoria_sin_comillas] = 1\n",
    "\n",
    "total_categorias = len(conteo_categorias)\n",
    "categorias_ordenadas = sorted(conteo_categorias.items(), key=lambda x: x[1])\n",
    "umbral = int(total_categorias * 0.8)\n",
    "categorias_filtradas = [\n",
    "    categoria for categoria, conteo in categorias_ordenadas if conteo >= umbral]\n",
    "df_restaurantes = df_restaurantes[\n",
    "    df_restaurantes[\"categorias\"].apply(\n",
    "        lambda x: any(cat in x for cat in categorias_filtradas))]\n",
    "df_merged = pd.merge(\n",
    "    df_reviews, df_restaurantes[[\"id_restaurante\"]], on=\"id_restaurante\", how=\"inner\")\n",
    "df_reviews = df_merged.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de Dataframe Estados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por necesidades del proyecto, se creará un dataframe con la información de los estados, para ser consultado, cargado y consumido.\n",
    "\n",
    "El dataset elegido para conseguir los datos de densidad de población contiene la población del último censo, del año 2020, debe actualizarse con el nuevo censo.\n",
    "\n",
    "En caso de faltar el archivo, se volverá a crear, sino, se leerá y actualizará los datos de población.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"Datalake/estados.parquet\"):\n",
    "    df_estados = pd.DataFrame({\n",
    "            \"State Abbreviation\": list(state_dictionary.keys()),\n",
    "            \"State\": [state_dictionary[abrv] for abrv in list(state_dictionary.keys())],\n",
    "            \"Population\": [0] * len(list(state_dictionary.keys())),})\n",
    "    df_poblacion = pd.read_csv(\"Datasets/us_pop_by_state.csv\")\n",
    "\n",
    "    for index, row in df_poblacion.iterrows():\n",
    "        state_code = row[\"state_code\"]\n",
    "        population = row[\"2020_census\"]\n",
    "        if state_code in df_estados[\"State Abbreviation\"].values:\n",
    "            df_estados.loc[\n",
    "                df_estados[\"State Abbreviation\"] == state_code, \"Population\"\n",
    "            ] = population\n",
    "\n",
    "    df_estados[\"id_estado\"] = range(1, len(df_estados) + 1)\n",
    "    df_estados = df_estados[[\"id_estado\", \"State\", \"State Abbreviation\", \"Population\"]]\n",
    "else:\n",
    "    df_estados = pd.read_parquet(\"Datalake/estados.parquet\")\n",
    "    df_poblacion = pd.read_csv(\"Datasets/us_pop_by_state.csv\")\n",
    "    for index, row in df_poblacion.iterrows():\n",
    "        state_code = row[\"state_code\"]\n",
    "        population = row[\"2020_census\"]\n",
    "        if state_code in df_estados[\"State Abbreviation\"].values:\n",
    "            df_estados.loc[\n",
    "                df_estados[\"State Abbreviation\"] == state_code, \"Population\"\n",
    "            ] = population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrección de Estados según Geolocalización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dictionary_inv = {v: k for k, v in state_dictionary.items()}\n",
    "\n",
    "\n",
    "def get_state_from_geojson(lat, lon, data_geo):\n",
    "    for feature in data_geo[\"features\"]:\n",
    "        polygon = shape(feature[\"geometry\"])\n",
    "        # Verificar las diferentes estructuras de propiedades\n",
    "        if \"NAME\" in feature[\"properties\"]:\n",
    "            state_name = feature[\"properties\"][\"NAME\"]\n",
    "        elif \"name\" in feature[\"properties\"]:\n",
    "            state_name = feature[\"properties\"][\"name\"]\n",
    "        else: continue\n",
    "        try: \n",
    "            if polygon.contains(Point(lon, lat)):\n",
    "                return state_dictionary_inv.get(state_name, None)\n",
    "        except TopologicalError as e:\n",
    "            print(f\"TopologicalError: {e} at ({lat}, {lon})\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_state_1(lat, lon): return get_state_from_geojson(lat, lon, data_geo_1)\n",
    "def get_state_2(lat, lon): return get_state_from_geojson(lat, lon, data_geo_2)\n",
    "\n",
    "\n",
    "def safe_get_state(row):\n",
    "    try: return get_state_1(row[\"latitud\"], row[\"longitud\"]) or get_state_2(\n",
    "            row[\"latitud\"], row[\"longitud\"])\n",
    "    except TopologicalError as e:\n",
    "        print(f\"Skipping row due to TopologicalError: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "df_restaurantes.loc[:, \"state_ab\"] = df_restaurantes.apply(safe_get_state, axis=1)\n",
    "df_restaurantes.drop(columns=[\"estado\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se mapearán los estados corregidos en df_restaurantes.state_ab con su contraparte en df_estados.id_estados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurantes[\"id_estado\"] = df_restaurantes[\"state_ab\"].map(\n",
    "    df_estados.set_index(\"State Abbreviation\")[\"id_estado\"])\n",
    "df_restaurantes.dropna(subset=[\"id_estado\"], inplace=True)\n",
    "df_restaurantes = df_restaurantes.iloc[:, [1, 9, 0, 2, 3, 4, 5, 6, 7, 8]].copy()\n",
    "df_restaurantes.drop(columns=[\"state_ab\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de Dataframes finales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversión de Dtypes para ahorrar memoria y almacenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurantes.loc[:, \"cantidad_resenas\"] = df_restaurantes[\"cantidad_resenas\"].astype(\n",
    "    \"uint32\")\n",
    "df_restaurantes.loc[:, \"calificacion\"] = df_restaurantes[\"calificacion\"].astype(\n",
    "    \"float16\")\n",
    "df_restaurantes.loc[:, \"id_estado\"] = df_restaurantes[\"id_estado\"].astype(\"uint8\")\n",
    "df_reviews.loc[:, \"calificacion\"] = df_reviews[\"calificacion\"].astype(\"float16\")\n",
    "df_reviews.loc[:, \"anio\"] = df_reviews[\"anio\"].astype(\"float16\")\n",
    "df_reviews.loc[:, \"puntaje_de_sentimiento\"] = df_reviews[\n",
    "    \"puntaje_de_sentimiento\"].astype(\"float16\")\n",
    "df_estados.loc[:, \"id_estado\"] = df_estados[\"id_estado\"].astype(\"uint8\")\n",
    "df_estados.loc[:, \"Population\"] = df_estados[\"Population\"].astype(\"uint32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_restaurantes_existente = pd.read_parquet(r\"Datalake\\restaurantes.parquet\")\n",
    "    df_reviews_existente = pd.read_parquet(r\"Datalake\\reviews.parquet\")\n",
    "except FileNotFoundError:\n",
    "    df_restaurantes_existente = pd.DataFrame()\n",
    "    df_reviews_existente = pd.DataFrame()\n",
    "\n",
    "df_restaurantes_concat = pd.concat([df_restaurantes_existente, df_restaurantes])\n",
    "df_reviews_concat = pd.concat([df_reviews_existente, df_reviews])\n",
    "\n",
    "df_restaurantes_concat[\"atributos\"] = df_restaurantes_concat[\"atributos\"].apply(str)\n",
    "df_restaurantes_concat.to_parquet(r\"Datalake\\restaurantes.parquet\")\n",
    "df_reviews_concat.to_parquet(r\"Datalake\\reviews.parquet\")\n",
    "df_estados.to_parquet(r\"Datalake\\estados.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
