{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de datos Datasets Google Maps y YELP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import geojson\n",
    "from shapely.geometry import shape, Point\n",
    "import shapely\n",
    "from shapely.geometry import shape, Point\n",
    "from shapely.errors import TopologicalError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiciones y carpetas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de diccionario de Estados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_abreviations = [\"AL\",\"AK\",\"AZ\",\"AR\",\"CA\",\"CO\",\"CT\",\"DE\",\"FL\",\"GA\",\"HI\",\"ID\",\"IL\",\"IN\",\"IA\",\"KS\",\"KY\",\"LA\",\"ME\",\"MD\",\"MA\",\"MI\",\"MN\",\"MS\",\"MO\",\"MT\",\"NE\",\"NV\",\"NH\",\"NJ\",\"NM\",\"NY\",\"NC\",\"ND\",\"OH\",\"OK\",\"OR\",\"PA\",\"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VT\",\"VA\",\"WA\",\"WV\",\"WI\",\"WY\",]\n",
    "state_dictionary = {\"AL\": \"Alabama\",\"AK\": \"Alaska\",\"AZ\": \"Arizona\",\"AR\": \"Arkansas\",\"CA\": \"California\",\"CO\": \"Colorado\",\"CT\": \"Connecticut\",\"DE\": \"Delaware\",\"FL\": \"Florida\",\"GA\": \"Georgia\",\"HI\": \"Hawaii\",\"ID\": \"Idaho\",\"IL\": \"Illinois\",\"IN\": \"Indiana\",\"IA\": \"Iowa\",\"KS\": \"Kansas\",\"KY\": \"Kentucky\",\"LA\": \"Louisiana\",\"ME\": \"Maine\",\"MD\": \"Maryland\",\"MA\": \"Massachusetts\",\"MI\": \"Michigan\",\"MN\": \"Minnesota\",\"MS\": \"Mississippi\",\"MO\": \"Missouri\",\"MT\": \"Montana\",\"NE\": \"Nebraska\",\"NV\": \"Nevada\",\"NH\": \"New Hampshire\",\"NJ\": \"New Jersey\",\"NM\": \"New Mexico\",\"NY\": \"New York\",\"NC\": \"North Carolina\",\"ND\": \"North Dakota\",\"OH\": \"Ohio\",\"OK\": \"Oklahoma\",\"OR\": \"Oregon\",\"PA\": \"Pennsylvania\",\"RI\": \"Rhode Island\",\"SC\": \"South Carolina\",\"SD\": \"South Dakota\",\"TN\": \"Tennessee\",\"TX\": \"Texas\",\"UT\": \"Utah\",\"VT\": \"Vermont\",\"VA\": \"Virginia\",\"WA\": \"Washington\",\"WV\": \"West Virginia\",\"WI\": \"Wisconsin\",\"WY\": \"Wyoming\",}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creación de directorios si no existen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"Generated\") if not os.path.exists(\"Generated\") else None\n",
    "os.chdir(\"Generated\")\n",
    "for subdir in [\"Google\", \"Yelp\", \"Unificado\"]:os.mkdir(subdir) if not os.path.exists(subdir) else None\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción con los datasets de Google Maps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metada de Sitios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se recorren los archivos línea por línea, durante el proceso se almacenan sólo las filas que incluyan <code>Restaurant</code> en la columna de categoría, así se optimiza el tamaño del dataset final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineas_json = []\n",
    "\n",
    "for i in range(1, 12):\n",
    "    path = f\"Datasets/Google Maps/metadata-sitios/{i}.json\"\n",
    "    with open(path, \"r\") as file:\n",
    "        for l in file:\n",
    "            try:\n",
    "                linea_j = json.loads(l)\n",
    "                if \"restaurant\" in \" \".join(linea_j[\"category\"]).lower():\n",
    "                    lineas_json.append(linea_j)\n",
    "            except: pass\n",
    "\n",
    "df = pd.DataFrame(lineas_json)\n",
    "df.to_parquet(r\"Generated\\Google\\metada_sitios.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tamaño Directorio <code>metadata-sitios</code>: 2.76 Gb\n",
    "\n",
    "Tamaño Archivo <code>metada_sitios.parquet</code>: 60.43 Mb\n",
    "\n",
    "Dimensiones: 212.014 filas x 15 Columnas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtención de información de Estados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base al campo <code>Address</code> obtenemos el estado donde se encuentra el negocio. Nos servirá para luego seleccionar los estados con más restaurantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_ab(st):\n",
    "    try:\n",
    "        state = st.split(\", \")[-1].split(\" \")[0]\n",
    "        if state in state_abreviations: return state\n",
    "        else: return np.nan\n",
    "    except: return np.nan\n",
    "\n",
    "df[\"state_ab\"] = df[\"address\"].apply(get_state_ab)\n",
    "top_5 = df[\"state_ab\"].value_counts().head(5).index.to_list()\n",
    "df[\"us_state\"] = df[\"state_ab\"].map(state_dictionary)\n",
    "\n",
    "top_5_url = [\n",
    "    f\"Datasets/Google Maps/reviews-estados/review-{state_dictionary[i].replace(' ', '_')}/\"\n",
    "    for i in top_5]\n",
    "\n",
    "cantidad_archivos = {}\n",
    "for i in top_5_url:\n",
    "    for j in os.walk(i):\n",
    "        cantidad_archivos[i] = len(j[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Estados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya con los estados elegidos estamos en condiciones de ingestar los datos de las carpetas correspondientes dentro del directorio <code>reviews-estados</code>.\n",
    "Es información masiva lo que genera un archivo de grandes dimensiones, sin embargo previamente filtramos por el parámetro de año <code>2017-2019</code> valiéndonos del campo <code>time</code>, que tiene es un <code>timestamp</code>, pero con 3 digitos más que el usado por <code>datetime</code> de Python. Le agregamos el campo <code>Estado</code> que es más descriptivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El archivo Datasets/Google Maps/reviews-estados/review-California/19.json no existe.\n",
      "El archivo Datasets/Google Maps/reviews-estados/review-Texas/17.json no existe.\n",
      "El archivo Datasets/Google Maps/reviews-estados/review-New_York/19.json no existe.\n",
      "El archivo Datasets/Google Maps/reviews-estados/review-Florida/20.json no existe.\n",
      "El archivo Datasets/Google Maps/reviews-estados/review-Pennsylvania/17.json no existe.\n"
     ]
    }
   ],
   "source": [
    "lineas_json_revs_google = []\n",
    "\n",
    "for i in top_5_url:\n",
    "    count = 0\n",
    "    for c in range(1, cantidad_archivos[i] + 1):\n",
    "        file_name = f\"{i}{c}.json\"\n",
    "        #file_name = f\"Datasets/Google Maps/reviews-estados/{i}{c}.json\"\n",
    "        if os.path.exists(file_name):\n",
    "            with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
    "                for s in f:\n",
    "                    linea = json.loads(s)\n",
    "                    linea[\"anio\"] = datetime.datetime.fromtimestamp(linea[\"time\"] / 1000).year\n",
    "                    linea[\"estado\"] = i.split(\"-\")[-1][:-1]\n",
    "                    if linea[\"anio\"] in [2017, 2018, 2019]:\n",
    "                        lineas_json_revs_google.append(linea)\n",
    "        else:\n",
    "            print(f\"El archivo {file_name} no existe.\")\n",
    "        count += 1\n",
    "\n",
    "df_revs_google = pd.DataFrame(lineas_json_revs_google)\n",
    "merge_site_reviews = pd.merge(df_revs_google, df, left_on=\"gmap_id\", right_on=\"gmap_id\")\n",
    "merge_site_reviews.to_parquet(r\"Generated\\Google\\merge_site_reviews.parquet\")\n",
    "df_revs_google.to_parquet(r\"Generated\\Google\\reviews-estados.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tamaño archivo: 760 Mb\n",
    "\n",
    "Tamaño dataset: 24.3 Gb\n",
    "\n",
    "Tamaño 8.339.179 filas x 10 Columnas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de los Dataset de YELP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contiene los datos de las entidades negocios de Yelp, a un primer vistazo tiene las columnas duplicadas, por lo que hay que hacer un recorte, ya que la segunda mitad tiene datos vacíos en su inmensa mayoría.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_business = r\"Datasets\\Yelp\\business.pkl\"\n",
    "df_business = pd.read_pickle(url_business)\n",
    "df_business = df_business.iloc[:, :-14]\n",
    "df_business = df_business[df_business.state.isin(top_5)]\n",
    "\n",
    "def is_restaurant(st):\n",
    "    try:\n",
    "        test = \"\".join(st).lower()\n",
    "        return \"restaurant\" in test\n",
    "    except: return False\n",
    "\n",
    "df_business = df_business[df_business[\"categories\"].apply(is_restaurant)]\n",
    "df_business.to_parquet(r\"Generated\\Yelp\\bussines.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se decidió que los datos de registros en el negocio por parte de los usuarios no era información pertinente para el proyecto, por lo que el archivo \"checkin.json\" no será cargado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se decidió que los datos de consejos de los usuarios no era información pertinente para el proyecto, por lo que el archivo \"tip.json\" no será cargado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_url = r\"Datasets\\Yelp\\review.json\"\n",
    "lineas_json_review = []\n",
    "\n",
    "with open(df_reviews_url, \"r\", encoding=\"utf-8\") as f:\n",
    "    count = 0\n",
    "    for i in f:\n",
    "        linea = json.loads(i)\n",
    "        anio = linea[\"date\"][:4]\n",
    "        if anio in [\"2017\", \"2018\", \"2019\"] and linea[\"useful\"] == 1:\n",
    "            lineas_json_review.append(linea)\n",
    "\n",
    "df_reviews = pd.DataFrame(lineas_json_review)\n",
    "df_reviews[\"funny\"] = df_reviews[\"funny\"].astype(\"int8\")\n",
    "df_reviews[\"stars\"] = df_reviews[\"stars\"].astype(\"int8\")\n",
    "df_reviews[\"cool\"] = df_reviews[\"cool\"].astype(\"int8\")\n",
    "df_reviews.drop(\"useful\", axis=1, inplace=True, errors=\"ignore\")\n",
    "\n",
    "df_reviews.to_parquet(r\"Generated\\Yelp\\review.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users Yelp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se decidió que los datos directamente relacionados a los usuarios no era información pertinente para el proyecto, por lo que el archivo \"users.parquet\" no será cargado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recarga de dataframes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza una carga de los dataframes exportados previamente, para acelerar el proceso de testeo, ya que los pasos anteriores requieren de un alto consumo de tiempo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maps_restaurantes = pd.read_parquet(r\"Generated\\Google\\metada_sitios.parquet\")\n",
    "df_maps_reviews = pd.read_parquet(r\"Generated\\Google\\merge_site_reviews.parquet\")\n",
    "df_yelp_restaurantes = pd.read_parquet(r\"Generated\\Yelp\\bussines.parquet\")\n",
    "df_yelp_reviews = pd.read_parquet(r\"Generated\\Yelp\\review.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de columnas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mantendrán sólo las columnas relacionadas al Modelo Entidad Relación, al mismo tiempo, serán renombradas para tener un formato unificado, que permita la unión en un sólo dataframe según las tablas del MER.\n",
    "\n",
    "- Se crea df_maps_reviews.review_id tomando los primeros 10 caracteres de gmap_id y user_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_maps_restaurantes = df_maps_restaurantes[[\"name\",\"gmap_id\",\"category\",\"num_of_reviews\",\"latitude\",\"longitude\",\"MISC\",\"avg_rating\",]]\n",
    "df_maps_restaurantes = df_maps_restaurantes.rename(\n",
    "    columns={\n",
    "        \"name\": \"nombre\",\n",
    "        \"gmap_id\": \"id_restaurante\",\n",
    "        \"category\": \"categorias\",\n",
    "        \"num_of_reviews\": \"cantidad_resenas\",\n",
    "        \"latitude\": \"latitud\",\n",
    "        \"longitude\": \"longitud\",\n",
    "        \"MISC\": \"atributos\",\n",
    "        \"avg_rating\": \"calificacion\",})\n",
    "\n",
    "df_yelp_restaurantes = df_yelp_restaurantes[[\"name\",\"business_id\",\"categories\",\"review_count\",\"latitude\",\"longitude\",\"attributes\",\"stars\",]]\n",
    "df_yelp_restaurantes = df_yelp_restaurantes.rename(\n",
    "    columns={\n",
    "        \"name\": \"nombre\",\n",
    "        \"business_id\": \"id_restaurante\",\n",
    "        \"categories\": \"categorias\",\n",
    "        \"review_count\": \"cantidad_resenas\",\n",
    "        \"latitude\": \"latitud\",\n",
    "        \"longitude\": \"longitud\",\n",
    "        \"attributes\": \"atributos\",\n",
    "        \"stars\": \"calificacion\",})\n",
    "\n",
    "df_maps_reviews[\"review_id\"] = (\n",
    "    df_maps_reviews[\"gmap_id\"].str[:10] + df_maps_reviews[\"user_id\"].str[:10])\n",
    "df_maps_reviews[\"sentiment_score\"] = 0\n",
    "df_maps_reviews = df_maps_reviews[[\"user_id\",\"gmap_id\",\"review_id\",\"rating\",\"anio\",\"sentiment_score\",]]\n",
    "df_maps_reviews = df_maps_reviews.rename(\n",
    "    columns={\n",
    "        \"user_id\": \"id_usuario\",\n",
    "        \"gmap_id\": \"id_restaurante\",\n",
    "        \"review_id\": \"id_resena\",\n",
    "        \"rating\": \"calificacion\",\n",
    "        \"anio\": \"anio\",\n",
    "        \"sentiment_score\": \"puntaje_de_sentimiento\",})\n",
    "\n",
    "df_yelp_reviews[\"anio\"] = df_yelp_reviews[\"date\"].str[:4]\n",
    "df_yelp_reviews[\"sentiment_score\"] = 0\n",
    "df_yelp_reviews = df_yelp_reviews[[\"user_id\",\"business_id\",\"review_id\",\"stars\",\"anio\",\"sentiment_score\",]]\n",
    "df_yelp_reviews = df_yelp_reviews.rename(\n",
    "    columns={\n",
    "        \"user_id\": \"id_usuario\",\n",
    "        \"business_id\": \"id_restaurante\",\n",
    "        \"review_id\": \"id_resena\",\n",
    "        \"stars\": \"calificacion\",\n",
    "        \"anio\": \"anio\",\n",
    "        \"sentiment_score\": \"puntaje_de_sentimiento\",})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unión de Dataframes y limpieza de duplicados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se unirán los Dataframes de restaurantes, y los dataframes de reviews, además de esto, se hará una revisión de los posibles registros duplicados y se eliminarán, esto apoyandose en los datos de longitud y latitud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurantes = pd.concat([df_yelp_restaurantes, df_maps_restaurantes])\n",
    "df_restaurantes = df_restaurantes.drop_duplicates(subset=[\"id_restaurante\"])\n",
    "# Corrección de columna categorias\n",
    "def convert_to_string(value):\n",
    "    if isinstance(value, np.ndarray): \n",
    "        return \", \".join(map(str, value))\n",
    "    return str(value)\n",
    "df_restaurantes[\"categorias\"] = df_restaurantes[\"categorias\"].apply(convert_to_string)\n",
    "\n",
    "df_reviews = pd.concat([df_yelp_reviews, df_maps_reviews])\n",
    "df_reviews = df_reviews.drop_duplicates(subset=[\"review_id\"])\n",
    "# corrección de columna anio\n",
    "df_reviews[\"anio\"] = df_reviews[\"anio\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de Dataframe Estados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "por necesidades del proyecto, se creará un dataframe con la información de los estados, para ser consultado, cargado y consumido.\n",
    "\n",
    "El dataset elegido para conseguir los datos de densidad de población contiene la población del último censo, del año 2020, debe actualizarse con el nuevo censo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estados = pd.DataFrame(\n",
    "    {\n",
    "        \"State Abbreviation\": state_abreviations,\n",
    "        \"State\": [state_dictionary[abrv] for abrv in state_abreviations],\n",
    "        \"Population\": [0] * len(state_abreviations),})\n",
    "\n",
    "df_poblacion = pd.read_csv(\"Datasets/us_pop_by_state.csv\")\n",
    "for index, row in df_poblacion.iterrows():\n",
    "    state_code = row[\"state_code\"]\n",
    "    population = row[\"2020_census\"]\n",
    "    if state_code in df_estados[\"State Abbreviation\"].values:\n",
    "        df_estados.loc[df_estados[\"State Abbreviation\"] == state_code, \"Population\"] = (\n",
    "            population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corrección de Estados según Geolocalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dictionary_inv = {v: k for k, v in state_dictionary.items()}\n",
    "\n",
    "def get_state_from_geojson(lat, lon, data_geo):\n",
    "    for feature in data_geo['features']:\n",
    "        polygon = shape(feature['geometry'])\n",
    "        # Verificar las diferentes estructuras de propiedades\n",
    "        if 'NAME' in feature['properties']:\n",
    "            state_name = feature['properties']['NAME']\n",
    "        elif 'name' in feature['properties']:\n",
    "            state_name = feature['properties']['name']\n",
    "        else:\n",
    "            continue\n",
    "        try:\n",
    "            if polygon.contains(Point(lon, lat)):\n",
    "                return state_dictionary_inv.get(state_name, None)\n",
    "        except TopologicalError as e:\n",
    "            print(f\"TopologicalError: {e} at ({lat}, {lon})\")\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_state_1(lat, lon): return get_state_from_geojson(lat, lon, data_geo_1)\n",
    "\n",
    "def get_state_2(lat, lon): return get_state_from_geojson(lat, lon, data_geo_2)\n",
    "\n",
    "def safe_get_state(row): \n",
    "    try: return get_state_1(row['latitud'], row['longitud']) or get_state_2(row['latitud'], row['longitud'])\n",
    "    except TopologicalError as e:\n",
    "        print(f\"Skipping row due to TopologicalError: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "GEOSException",
     "evalue": "TopologyException: side location conflict at -135.61113979463926 57.425339066329983. This can occur if the input geometry is invalid.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGEOSException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_restaurantes[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_ab\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_restaurantes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43msafe_get_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_restaurantes\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[60], line 27\u001b[0m, in \u001b[0;36msafe_get_state\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msafe_get_state\u001b[39m(row): \n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_state_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlatitud\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlongitud\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m get_state_2(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitud\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitud\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TopologicalError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping row due to TopologicalError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[60], line 22\u001b[0m, in \u001b[0;36mget_state_1\u001b[1;34m(lat, lon)\u001b[0m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_1\u001b[39m(lat, lon): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_state_from_geojson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_geo_1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[60], line 14\u001b[0m, in \u001b[0;36mget_state_from_geojson\u001b[1;34m(lat, lon, data_geo)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpolygon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m state_dictionary_inv\u001b[38;5;241m.\u001b[39mget(state_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TopologicalError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\shapely\\geometry\\base.py:675\u001b[0m, in \u001b[0;36mBaseGeometry.contains\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m    674\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if the geometry contains the other, else False\"\"\"\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_unpack(\u001b[43mshapely\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\shapely\\decorators.py:77\u001b[0m, in \u001b[0;36mmultithreading_enabled.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[0;32m     76\u001b[0m         arr\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mwriteable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\shapely\\predicates.py:526\u001b[0m, in \u001b[0;36mcontains\u001b[1;34m(a, b, **kwargs)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;129m@multithreading_enabled\u001b[39m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontains\u001b[39m(a, b, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    474\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns True if geometry B is completely inside geometry A.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \n\u001b[0;32m    476\u001b[0m \u001b[38;5;124;03m    A contains B if no points of B lie in the exterior of A and at least one\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontains\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mGEOSException\u001b[0m: TopologyException: side location conflict at -135.61113979463926 57.425339066329983. This can occur if the input geometry is invalid."
     ]
    }
   ],
   "source": [
    "df_restaurantes['state_ab'] = df_restaurantes.apply(safe_get_state, axis=1)\n",
    "print(df_restaurantes.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracción de Dataframes finales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_restaurantes_existente = pd.read_parquet(r\"Generated\\Unificado\\restaurantes.parquet\")\n",
    "    df_reviews_existente = pd.read_parquet(r\"Generated\\Unificado\\reviews.parquet\")\n",
    "except FileNotFoundError:\n",
    "    df_restaurantes_existente = pd.DataFrame()\n",
    "    df_reviews_existente = pd.DataFrame()\n",
    "\n",
    "df_restaurantes_concat = pd.concat([df_restaurantes_existente, df_restaurantes])\n",
    "df_reviews_concat = pd.concat([df_reviews_existente, df_reviews])\n",
    "\n",
    "df_restaurantes_concat.to_parquet(r\"Generated\\Unificado\\restaurantes.parquet\")\n",
    "df_reviews_concat.to_parquet(r\"Generated\\Unificado\\reviews.parquet\")\n",
    "df_estados.to_parquet(r\"Generated\\Unificado\\estados.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
